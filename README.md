# AssessorAI Crawler

Sistema de web scraping para coleta automatizada de proposiÃ§Ãµes legislativas de diversas casas legislativas brasileiras.

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#sobre-o-projeto)
- [Arquitetura](#arquitetura)
- [Spiders DisponÃ­veis](#spiders-disponÃ­veis)
- [Requisitos](#requisitos)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Uso](#uso)
- [Estrutura de Dados](#estrutura-de-dados)
- [Desenvolvimento](#desenvolvimento)
- [ManutenÃ§Ã£o](#manutenÃ§Ã£o)

## ğŸ¯ Sobre o Projeto

O AssessorAI Crawler Ã© uma soluÃ§Ã£o baseada em Scrapy para extraÃ§Ã£o de proposiÃ§Ãµes legislativas de mÃºltiplas casas legislativas brasileiras. O sistema utiliza Scrapyd para gerenciamento de spiders em produÃ§Ã£o e ScrapydWeb para interface de monitoramento.

### Principais Funcionalidades

- âœ… Scraping automatizado de proposiÃ§Ãµes legislativas
- âœ… Suporte a mÃºltiplas casas legislativas (estaduais e municipais)
- âœ… Interface web para gerenciamento (ScrapydWeb)
- âœ… Download de arquivos associados (PDFs, imagens, etc.)
- âœ… Sistema de validaÃ§Ã£o e pipeline de processamento
- âœ… Armazenamento persistente de dados e logs
- âœ… Proxy reverso Nginx para acesso unificado

## ğŸ—ï¸ Arquitetura

O sistema Ã© composto por 4 containers Docker orquestrados via Docker Compose:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NGINX                        â”‚
â”‚            (Proxy Reverso - :80)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  SCRAPYD  â”‚             â”‚  SCRAPYDWEB    â”‚
    â”‚   :6800   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     :5000      â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ LOGPARSER â”‚
    â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Containers

- **nginx**: Proxy reverso para acesso unificado aos serviÃ§os
- **scrapyd**: Daemon do Scrapy para execuÃ§Ã£o dos spiders
- **scrapydweb**: Interface web para gerenciamento e monitoramento
- **logparser**: Parser de logs do Scrapyd

### Volumes

- **scrapyd-eggs**: Armazena projetos deployados
- **./storage**: DiretÃ³rio local com todos os dados persistentes
  - `logs/`: Logs de execuÃ§Ã£o dos spiders
  - `items/`: Items extraÃ­dos em formato JSON
  - `dbs/`: Databases SQLite
  - `downloads/`: Arquivos baixados pelos spiders

## ğŸ•·ï¸ Spiders DisponÃ­veis

### Ã‚mbito Federal

| Spider | Nome | Casa Legislativa |
|--------|------|------------------|
| `proposicoescn` | ProposicoesCNSpider | Congresso Nacional |
| `proposicoespcd` | ProposicoesPCDSpider | CÃ¢mara dos Deputados |

### Ã‚mbito Estadual

| Spider | Nome | Estado |
|--------|------|--------|
| `proposicoesba` | ProposicoesBASpider | Bahia (ALBA) |
| `proposicoesmg` | ProposicoesMGSpider | Minas Gerais (ALMG) |
| `proposicoespe` | ProposicoesPESpider | Pernambuco (ALEPE) |
| `proposicoespr` | ProposicoesPRSpider | ParanÃ¡ (ALEP) |
| `proposicoesrs` | ProposicoesRSSpider | Rio Grande do Sul (ALRS) |
| `proposicoessc` | ProposicoesSCSpider | Santa Catarina (ALESC) |
| `proposicoessp` | ProposicoesSPSpider | SÃ£o Paulo (ALESP) |

### Ã‚mbito Municipal

| Spider | Nome | MunicÃ­pio |
|--------|------|-----------|
| `proposicoescidsp` | ProposicoescidspSpider | SÃ£o Paulo (CMSP) |
| `proposicoesfortaleza` | ProposicoesFortalezaSpider | Fortaleza |
| `proposicoeslinhares` | ProposicoesLinharesSpider | Linhares |
| `proposicoespocosdecaldas` | ProposicoesPocosDeCaldasSpider | PoÃ§os de Caldas |
| `proposicoessjc` | ProposicoesSJCSpider | SÃ£o JosÃ© dos Campos |

## ğŸ“¦ Requisitos

- Docker 20.10+
- Docker Compose 2.0+
- 2GB RAM mÃ­nimo
- 10GB espaÃ§o em disco (recomendado)

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/pmarkun/assessorai_crawler.git
cd assessorai_crawler
```

### 2. Configure as VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```bash
cp .env.example .env
```

Edite o arquivo `.env` com suas configuraÃ§Ãµes:

```env
# Exemplo de variÃ¡veis de ambiente
WEAVIATE_URL=http://weaviate:8080
GOOGLE_API_KEY=your_api_key_here
```

### 3. Crie a Estrutura de Storage

```bash
mkdir -p storage/{logs,items,dbs,downloads}
```

### 4. Inicie os Containers

```bash
docker compose up -d
```

### 5. Verifique o Status

```bash
docker compose ps
```

Todos os containers devem estar com status `Up`.

## ğŸ’» Uso

### Interface Web

Acesse o ScrapydWeb atravÃ©s do navegador:

```
http://localhost
```

### Executar um Spider via Interface

1. Acesse http://localhost
2. Navegue atÃ© "Jobs" â†’ "Run"
3. Selecione o spider desejado
4. Configure os parÃ¢metros (se necessÃ¡rio)
5. Clique em "Run"

### Executar um Spider via API

```bash
# Listar spiders disponÃ­veis
curl http://localhost/scrapyd/listspiders.json?project=default

# Executar um spider
curl http://localhost/scrapyd/schedule.json \
  -d project=default \
  -d spider=proposicoesmg

# Verificar status
curl http://localhost/scrapyd/listjobs.json?project=default
```

### Executar um Spider via CLI (Local)

```bash
# Dentro do container
docker exec -it assessorai-scrapyd bash
scrapy crawl proposicoesmg

# Ou diretamente
docker exec -it assessorai-scrapyd scrapy crawl proposicoesmg
```

### Executar MÃºltiplos Spiders

```bash
# Script para executar todos os spiders
for spider in proposicoesba proposicoesmg proposicoessp; do
  curl http://localhost/scrapyd/schedule.json \
    -d project=default \
    -d spider=$spider
done
```

## ğŸ“Š Estrutura de Dados

### Item de ProposiÃ§Ã£o

Os items extraÃ­dos seguem uma estrutura padrÃ£o:

```json
{
  "id": "string",
  "numero": "string",
  "ano": "integer",
  "tipo": "string",
  "ementa": "string",
  "autor": "string",
  "data_apresentacao": "string (YYYY-MM-DD)",
  "situacao": "string",
  "url": "string",
  "url_inteiro_teor": "string",
  "files": ["array de URLs de arquivos"],
  "origem": "string (fonte dos dados)"
}
```

### LocalizaÃ§Ã£o dos Dados

```
storage/
â”œâ”€â”€ items/
â”‚   â””â”€â”€ default/
â”‚       â”œâ”€â”€ proposicoesmg/
â”‚       â”‚   â””â”€â”€ items-YYYY-MM-DD_HH-MM-SS.jl
â”‚       â””â”€â”€ proposicoessp/
â”‚           â””â”€â”€ items-YYYY-MM-DD_HH-MM-SS.jl
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ default/
â”‚       â””â”€â”€ proposicoesmg/
â”‚           â””â”€â”€ log-YYYY-MM-DD_HH-MM-SS.log
â””â”€â”€ downloads/
    â””â”€â”€ full/
        â””â”€â”€ [hash]/
            â””â”€â”€ arquivo.pdf
```

## ğŸ”§ Desenvolvimento

### Estrutura do Projeto

```
assessorai_crawler/
â”œâ”€â”€ assessorai_crawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py           # DefiniÃ§Ã£o dos items
â”‚   â”œâ”€â”€ middlewares.py     # Middlewares customizados
â”‚   â”œâ”€â”€ pipelines.py       # Pipelines de processamento
â”‚   â”œâ”€â”€ settings.py        # ConfiguraÃ§Ãµes do Scrapy
â”‚   â”œâ”€â”€ utils.py           # FunÃ§Ãµes utilitÃ¡rias
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ proposicoeslegislapi.py  # Spider base para APIs Legislativas
â”‚       â””â”€â”€ [outros spiders].py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scrapy.cfg
â”œâ”€â”€ scrapyd.conf
â””â”€â”€ nginx.conf
```

### Criar um Novo Spider

```bash
# Dentro do container
docker exec -it assessorai-scrapyd bash
scrapy genspider nome_spider domain.com

# Ou localmente (se tiver Scrapy instalado)
scrapy genspider nome_spider domain.com
```

### Testar um Spider

```bash
# Teste rÃ¡pido (10 items)
docker exec -it assessorai-scrapyd scrapy crawl proposicoesmg -s CLOSESPIDER_ITEMCOUNT=10

# Com output em arquivo
docker exec -it assessorai-scrapyd scrapy crawl proposicoesmg -o /app/storage/items/test.json

# Com logs detalhados
docker exec -it assessorai-scrapyd scrapy crawl proposicoesmg -L DEBUG
```

### Deploy de AlteraÃ§Ãµes

ApÃ³s modificar o cÃ³digo:

```bash
# Rebuild e restart
docker compose down
docker compose build
docker compose up -d
```

Ou apenas restart (se montou o cÃ³digo como volume):

```bash
docker compose restart scrapyd
```

## ğŸ› ï¸ ManutenÃ§Ã£o

### Visualizar Logs

```bash
# Logs do container
docker compose logs -f scrapyd

# Logs especÃ­ficos de um spider
docker exec -it assessorai-scrapyd cat /app/storage/logs/default/proposicoesmg/latest.log

# Via interface web
# http://localhost/scrapyd/logs/
```

### Limpar Dados Antigos

```bash
# Limpar logs com mais de 30 dias
find storage/logs -type f -mtime +30 -delete

# Limpar items processados
rm -rf storage/items/default/*/items-*.jl
```

### Backup

```bash
# Backup completo do storage
tar czf backup-storage-$(date +%Y%m%d).tar.gz storage/

# Backup apenas dos items
tar czf backup-items-$(date +%Y%m%d).tar.gz storage/items/
```

### Monitoramento

```bash
# Status dos containers
docker compose ps

# Uso de recursos
docker stats

# Jobs em execuÃ§Ã£o
curl http://localhost/scrapyd/listjobs.json?project=default | jq
```

### Atualizar DependÃªncias

```bash
# Edite requirements.txt
vim requirements.txt

# Rebuild da imagem
docker compose build

# Restart dos containers
docker compose down
docker compose up -d
```

## ğŸ” Troubleshooting

### Container nÃ£o inicia

```bash
# Verificar logs
docker compose logs scrapyd

# Verificar configuraÃ§Ã£o
docker compose config
```

### Spider falha ao executar

```bash
# Verificar logs detalhados
docker exec -it assessorai-scrapyd scrapy crawl proposicoesmg -L DEBUG

# Verificar conectividade
docker exec -it assessorai-scrapyd curl -I https://www.almg.gov.br
```

### Storage cheio

```bash
# Verificar uso de disco
du -sh storage/*

# Limpar logs antigos
docker exec -it assessorai-scrapyd find /app/storage/logs -mtime +7 -delete
```

### Resetar ambiente

```bash
# CUIDADO: Remove todos os dados!
docker compose down -v
rm -rf storage/*
mkdir -p storage/{logs,items,dbs,downloads}
docker compose up -d
```

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a [especificar licenÃ§a].

## ğŸ‘¥ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“§ Contato

Projeto AssessorAI - [@pmarkun](https://github.com/pmarkun)

---

**Nota**: Este projeto faz parte da iniciativa AssessorAI para democratizaÃ§Ã£o do acesso a dados legislativos brasileiros.
