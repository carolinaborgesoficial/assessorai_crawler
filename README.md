# AssessorAI Crawler

Sistema de crawler para extra√ß√£o de proposi√ß√µes legislativas de assembleias estaduais brasileiras, desenvolvido com Scrapy e integra√ß√£o com Weaviate.

## üèóÔ∏è Arquitetura do Projeto

```
assessorai_crawler/
‚îú‚îÄ‚îÄ assessorai_crawler/          # C√≥digo principal do Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ spiders/                 # Spiders para cada estado
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proposicoeslegislapi.py  # Spider base (classe pai)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proposicoessp.py         # S√£o Paulo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proposicoesmg.py         # Minas Gerais
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...                      # Outros estados
‚îÇ   ‚îú‚îÄ‚îÄ items.py                 # Defini√ß√£o dos dados estruturados
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py             # Processamento e valida√ß√£o dos dados
‚îÇ   ‚îú‚îÄ‚îÄ settings.py              # Configura√ß√µes do Scrapy
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                 # Fun√ß√µes utilit√°rias
‚îú‚îÄ‚îÄ output/                      # JSONs gerados pelos crawlers
‚îú‚îÄ‚îÄ importer.py                  # Script para importar dados no Weaviate
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias Python
‚îî‚îÄ‚îÄ .env                        # Vari√°veis de ambiente
```

## üöÄ Configura√ß√£o do Ambiente

### 1. Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone <repo-url>
cd assessorai_crawler

# Crie e ative um ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows

# Instale as depend√™ncias
pip install -r requirements.txt
```

### 2. Configura√ß√£o das Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# Weaviate Configuration
WEAVIATE_URL="your-weaviate-cluster-url"
WEAVIATE_APIKEY="your-weaviate-api-key"
WEAVIATE_CLASS="Bill"

# OpenAI Configuration (para embeddings)
OPENAI_APIKEY="your-openai-api-key"
```

## üìä Como Funciona

### 1. Estrutura de Dados

O projeto usa o item `ProposicaoItem` definido em `items.py`:

```python
ProposicaoItem:
- title: str          # T√≠tulo da proposi√ß√£o
- house: str          # Casa legislativa
- type: str           # Tipo (PL, PEC, etc.)
- number: int         # N√∫mero da proposi√ß√£o
- year: int           # Ano
- author: list        # Lista de autores
- subject: str        # Ementa/assunto
- full_text: str      # Texto completo
- url: str            # URL p√∫blica
- uuid: str           # Identificador √∫nico
- scraped_at: str     # Timestamp da coleta
```

### 2. Pipeline de Processamento

1. **ValidationPipeline**: Valida campos obrigat√≥rios
2. **JsonWriterSinglePipeline**: Salva todos os itens em um √∫nico JSON

## üï∑Ô∏è Como Desenvolver um Novo Crawler Web

### Metodologia: Do Site √† Estrutura de Dados

1. **üîç Encontre a p√°gina da casa legislativa**
   - Identifique o site oficial (ex: `www.al[uf].gov.br`)
   - Localize a se√ß√£o de "Proposi√ß√µes", "Projetos de Lei" ou similar

2. **üìã Encontre a p√°gina que lista os projetos**
   - Busque por p√°ginas de listagem (ex: `/proposicoes`, `/projetos`)
   - Analise a pagina√ß√£o e filtros dispon√≠veis

3. **üîó Itere pela p√°gina, buscando links para projetos individuais**
   - Identifique os seletores CSS/XPath dos links
   - Colete metadados b√°sicos da listagem (t√≠tulo, autor, data)

4. **üíæ Armazene as vari√°veis necess√°rias**
   - T√≠tulo da proposi√ß√£o
   - Tipo e n√∫mero (PL, PEC, etc.)
   - Autor(es)
   - Data de apresenta√ß√£o
   - Ementa/assunto
   - URL p√∫blica

5. **üìÑ Fa√ßa download da √≠ntegra e converta para markdown**
   - Acesse p√°gina individual do projeto
   - Extraia o texto completo (PDF, HTML, DOC)
   - Converta para markdown limpo

### Passo 1: Estrutura B√°sica do Spider

```python
# assessorai_crawler/spiders/proposicoes[uf].py
import scrapy
import hashlib
from datetime import datetime
from urllib.parse import urljoin
from ..items import ProposicaoItem

class Proposicoes[UF]Spider(scrapy.Spider):
    name = 'proposicoes[uf]'
    house = 'Nome da Casa Legislativa'
    uf = '[uf]'
    slug = f'proposicoes{uf}'
    allowed_domains = ['www.al[uf].gov.br']
    start_urls = ['https://www.al[uf].gov.br/proposicoes']
    
    def parse(self, response):
        """Parse da p√°gina de listagem de proposi√ß√µes"""
        # Extrair links para proposi√ß√µes individuais
        proposicao_links = response.css('selector-para-links::attr(href)').getall()
        
        for link in proposicao_links:
            full_url = urljoin(response.url, link)
            yield response.follow(full_url, self.parse_proposicao)
        
        # Pagina√ß√£o
        next_page = response.css('selector-proxima-pagina::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_proposicao(self, response):
        """Parse da p√°gina individual da proposi√ß√£o"""
        item = ProposicaoItem()
        
        # Extrair dados b√°sicos
        item['title'] = response.css('h1.titulo::text').get('').strip()
        item['house'] = self.house
        item['url'] = response.url
        
        # Extrair tipo e n√∫mero do t√≠tulo
        title_parts = item['title'].split()
        item['type'] = title_parts[0] if title_parts else ''
        
        # Extrair n√∫mero e ano (formato: "123/2024")
        if len(title_parts) > 1:
            try:
                num_year = title_parts[1].split('/')
                item['number'] = int(num_year[0])
                item['year'] = int(num_year[1])
            except (ValueError, IndexError):
                item['number'] = None
                item['year'] = None
        
        # Extrair outros campos
        item['author'] = self.extract_authors(response)
        item['subject'] = response.css('.ementa::text').get('').strip()
        item['presentation_date'] = self.extract_date(response)
        
        # Extrair texto completo
        texto_completo = self.extract_full_text(response)
        item['full_text'] = self.convert_to_markdown(texto_completo)
        item['length'] = len(item['full_text'] or '')
        
        # Metadados
        item['uuid'] = hashlib.md5(item['title'].encode('utf-8')).hexdigest()
        item['scraped_at'] = datetime.now().isoformat()
        
        yield item
    
    def extract_authors(self, response):
        """Extrai lista de autores"""
        authors_text = response.css('.autores::text').get('')
        return [a.strip() for a in authors_text.split(',') if a.strip()]
    
    def extract_date(self, response):
        """Extrai data de apresenta√ß√£o"""
        date_text = response.css('.data-apresentacao::text').get('')
        # Implementar parsing de data espec√≠fico do site
        return date_text.strip()
    
    def extract_full_text(self, response):
        """Extrai texto completo da proposi√ß√£o"""
        # M√©todo 1: Texto direto na p√°gina
        full_text = response.css('.texto-completo').get()
        if full_text:
            return full_text
        
        # M√©todo 2: Link para PDF/DOC
        pdf_link = response.css('a[href*=".pdf"]::attr(href)').get()
        if pdf_link:
            # Fazer request para PDF e processar (ver se√ß√£o de bibliotecas)
            pass
        
        return ''
    
    def convert_to_markdown(self, html_content):
        """Converte HTML para markdown limpo"""
        if not html_content:
            return ''
        
        # Usar biblioteca de convers√£o (ver se√ß√£o de bibliotecas)
        # Exemplo com html2text
        import html2text
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = True
        return h.handle(html_content)
```

### Passo 2: Bibliotecas √öteis

Adicione ao `requirements.txt`:

```txt
# Parsing e extra√ß√£o
beautifulsoup4          # Parsing HTML avan√ßado
lxml                    # Parser XML/HTML r√°pido
selectolax              # Parser HTML ultrarr√°pido

# Convers√£o de documentos
html2text               # HTML para Markdown
markdownify             # HTML para Markdown (alternativa)
pypandoc                # Convers√£o universal de documentos

# Processamento de PDF
PyPDF2                  # Extra√ß√£o de texto de PDF
pdfplumber              # PDF parsing avan√ßado
pymupdf                 # PDF processing (fitz)

# Processamento de texto
bleach                  # Limpeza de HTML
textract                # Extra√ß√£o de texto de v√°rios formatos

# Utilidades web
requests-html           # Requests com suporte a JavaScript
selenium                # Automa√ß√£o de browser (para SPAs)
playwright              # Alternativa moderna ao Selenium
```

### Passo 3: Pseudoc√≥digo Detalhado

```python
def develop_new_crawler():
    """
    Fluxo completo para desenvolver crawler de nova casa legislativa
    """
    
    # FASE 1: RECONHECIMENTO
    target_site = identify_legislative_house_website()
    propositions_section = find_propositions_listing_page(target_site)
    
    # FASE 2: AN√ÅLISE DA ESTRUTURA
    pagination_pattern = analyze_pagination(propositions_section)
    list_item_selectors = identify_list_item_selectors(propositions_section)
    individual_page_pattern = analyze_individual_pages(propositions_section)
    
    # FASE 3: EXTRA√á√ÉO DE METADADOS
    for page in paginate_through_listings(propositions_section):
        for item_link in extract_proposition_links(page):
            metadata = extract_basic_info_from_listing(item_link)
            
            # FASE 4: EXTRA√á√ÉO DE CONTE√öDO COMPLETO
            individual_page = fetch_individual_page(item_link)
            full_content = extract_full_content(individual_page)
            
            # FASE 5: PROCESSAMENTO E LIMPEZA
            cleaned_content = clean_and_normalize_text(full_content)
            markdown_content = convert_to_markdown(cleaned_content)
            
            # FASE 6: ESTRUTURA√á√ÉO DE DADOS
            proposition_item = create_proposition_item(
                title=metadata['title'],
                house=target_site['house_name'],
                authors=metadata['authors'],
                date=metadata['date'],
                full_text=markdown_content,
                url=item_link
            )
            
            yield proposition_item

def extract_full_content(page_response):
    """Estrat√©gias para extrair texto completo"""
    
    # ESTRAT√âGIA 1: Texto direto na p√°gina HTML
    if has_direct_text_content(page_response):
        return extract_html_text(page_response)
    
    # ESTRAT√âGIA 2: Download de PDF
    elif has_pdf_link(page_response):
        pdf_url = get_pdf_link(page_response)
        pdf_content = download_and_extract_pdf(pdf_url)
        return pdf_content
    
    # ESTRAT√âGIA 3: Documento Word/DOC
    elif has_doc_link(page_response):
        doc_url = get_doc_link(page_response)
        doc_content = download_and_extract_doc(doc_url)
        return doc_content
    
    # ESTRAT√âGIA 4: Conte√∫do carregado via JavaScript
    elif requires_javascript(page_response):
        js_content = extract_with_selenium(page_response.url)
        return js_content
    
    return ""
```

### Passo 4: Implementa√ß√µes Espec√≠ficas por Tipo de Conte√∫do

```python
# Para sites com PDF
def extract_pdf_content(pdf_url):
    """Extrai texto de PDF usando pdfplumber"""
    import pdfplumber
    import requests
    
    response = requests.get(pdf_url)
    with pdfplumber.open(BytesIO(response.content)) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Para sites com JavaScript/SPA
def extract_with_selenium(url):
    """Extrai conte√∫do de sites com JavaScript"""
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    
    options = Options()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)
    
    driver.get(url)
    # Aguardar carregamento
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "content"))
    )
    
    content = driver.find_element(By.CLASS_NAME, "texto-completo").text
    driver.quit()
    return content

# Para limpeza e convers√£o
def clean_and_convert_to_markdown(html_content):
    """Limpa HTML e converte para markdown"""
    import bleach
    import html2text
    
    # Limpar HTML malicioso/desnecess√°rio
    clean_html = bleach.clean(
        html_content,
        tags=['p', 'br', 'strong', 'em', 'ul', 'ol', 'li', 'h1', 'h2', 'h3'],
        strip=True
    )
    
    # Converter para markdown
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.ignore_images = True
    h.body_width = 0  # Sem quebra de linha
    
    markdown = h.handle(clean_html)
    
    # Limpeza adicional
    markdown = re.sub(r'\n\n+', '\n\n', markdown)  # M√∫ltiplas quebras
    markdown = markdown.strip()
    
    return markdown
```

### Passo 5: Ferramentas de Desenvolvimento e Debug

```python
# Ferramenta para an√°lise de seletores CSS
def analyze_page_structure(url):
    """Analisa estrutura da p√°gina para identificar seletores"""
    import requests
    from bs4 import BeautifulSoup
    
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Identificar poss√≠veis seletores para proposi√ß√µes
    potential_selectors = [
        'a[href*="proposicao"]',
        'a[href*="projeto"]', 
        'a[href*="pl"]',
        '.proposicao-item a',
        '.projeto-link',
        'tr td a'  # Para tabelas
    ]
    
    for selector in potential_selectors:
        elements = soup.select(selector)
        if elements:
            print(f"Selector '{selector}' encontrou {len(elements)} elementos")
            for i, elem in enumerate(elements[:3]):  # Primeiros 3
                print(f"  {i+1}: {elem.get('href')} - {elem.text.strip()}")

# Ferramenta para testar extra√ß√£o
def test_extraction(url, selectors_dict):
    """Testa seletores em uma p√°gina espec√≠fica"""
    import requests
    from bs4 import BeautifulSoup
    
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    results = {}
    for field, selector in selectors_dict.items():
        try:
            element = soup.select_one(selector)
            results[field] = element.text.strip() if element else 'NOT FOUND'
        except Exception as e:
            results[field] = f'ERROR: {str(e)}'
    
    return results

# Exemplo de uso das ferramentas
if __name__ == "__main__":
    # Analisar estrutura da p√°gina de listagem
    analyze_page_structure("https://www.alxx.gov.br/proposicoes")
    
    # Testar extra√ß√£o em p√°gina individual
    test_selectors = {
        'title': 'h1.titulo',
        'authors': '.autores',
        'date': '.data-apresentacao',
        'subject': '.ementa',
        'full_text': '.texto-completo'
    }
    
    results = test_extraction(
        "https://www.alxx.gov.br/proposicao/123", 
        test_selectors
    )
    
    for field, value in results.items():
        print(f"{field}: {value}")
```

### Passo 6: Estrat√©gias por Tipo de Site

```python
# TIPO 1: Sites est√°ticos simples (HTML tradicional)
class SimpleHTMLSpider(scrapy.Spider):
    """Para sites com HTML est√°tico e estrutura simples"""
    
    def parse_static_listing(self, response):
        # Seletores diretos funcionam bem
        links = response.css('a.proposicao-link::attr(href)').getall()
        for link in links:
            yield response.follow(link, self.parse_proposicao)

# TIPO 2: Sites com pagina√ß√£o AJAX
class AjaxPaginationSpider(scrapy.Spider):
    """Para sites que carregam mais conte√∫do via AJAX"""
    
    def parse_ajax_pagination(self, response):
        # Interceptar requests AJAX
        import json
        
        # Primeira p√°gina normal
        yield from self.parse_static_listing(response)
        
        # P√°ginas AJAX subsequentes
        ajax_url = "https://site.gov.br/api/proposicoes"
        for page in range(2, 100):  # Ajustar limite
            yield scrapy.Request(
                f"{ajax_url}?page={page}",
                callback=self.parse_ajax_response,
                headers={'X-Requested-With': 'XMLHttpRequest'}
            )
    
    def parse_ajax_response(self, response):
        data = json.loads(response.text)
        for item in data.get('items', []):
            yield response.follow(item['url'], self.parse_proposicao)

# TIPO 3: Sites Single Page Application (SPA)
class SPASpider(scrapy.Spider):
    """Para sites React/Vue/Angular"""
    
    def __init__(self):
        # Requer Selenium ou Playwright
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        options = Options()
        options.add_argument('--headless')
        self.driver = webdriver.Chrome(options=options)
    
    def parse_spa_content(self, response):
        self.driver.get(response.url)
        
        # Aguardar carregamento
        WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "proposicao-item"))
        )
        
        # Extrair links ap√≥s JavaScript executar
        elements = self.driver.find_elements(By.CSS_SELECTOR, "a.proposicao-link")
        for element in elements:
            url = element.get_attribute('href')
            yield scrapy.Request(url, callback=self.parse_proposicao)
    
    def closed(self, reason):
        self.driver.quit()
```

## üèÉ‚Äç‚ôÇÔ∏è Executando os Crawlers

### Executar um Spider Espec√≠fico

```bash
# Executar spider de S√£o Paulo
scrapy crawl proposicoessp

# Executar spider de Minas Gerais
scrapy crawl proposicoesmg

# Ver lista de todos os spiders
scrapy list
```

### Executar com Configura√ß√µes Espec√≠ficas

```bash
# Salvar em formato espec√≠fico
scrapy crawl proposicoessp -o output/sp_dados.json

# Executar com log espec√≠fico
scrapy crawl proposicoessp -L INFO

# Executar em modo debug
scrapy crawl proposicoessp -L DEBUG

# Limitar n√∫mero de itens (para testes)
scrapy crawl proposicoessp -s CLOSESPIDER_ITEMCOUNT=10

# Configurar delay entre requests
scrapy crawl proposicoessp -s DOWNLOAD_DELAY=2
```

### Exemplos Pr√°ticos de Desenvolvimento

```bash
# 1. Criar novo spider interativamente
scrapy genspider proposicoesgo www.assembleia.go.gov.br

# 2. Testar seletores com scrapy shell
scrapy shell "https://www.assembleia.go.gov.br/proposicoes"

# 3. Debug espec√≠fico de uma p√°gina
scrapy shell "https://www.assembleia.go.gov.br/proposicao/12345"

# 4. Executar com configura√ß√µes de desenvolvimento
scrapy crawl proposicoesgo \
  -s DOWNLOAD_DELAY=1 \
  -s CLOSESPIDER_ITEMCOUNT=5 \
  -L DEBUG
```

### Comandos √öteis no Scrapy Shell

```python
# No scrapy shell, use estes comandos para testar:

# Testar seletores CSS
response.css('a.proposicao-link').getall()
response.css('h1.titulo::text').get()

# Testar XPath
response.xpath('//a[contains(@href, "proposicao")]/@href').getall()

# Seguir link e testar
fetch('https://site.gov.br/proposicao/123')
response.css('.texto-completo::text').get()

# Testar regex
import re
title = "PL 123/2024"
match = re.match(r'(\w+)\s+(\d+)/(\d+)', title)
if match:
    print(f"Tipo: {match.group(1)}, N√∫mero: {match.group(2)}, Ano: {match.group(3)}")
```

## üì§ Importando Dados para o Weaviate

Ap√≥s executar os crawlers, use o script de importa√ß√£o:

```bash
# Importar dados de um estado espec√≠fico
python importer.py output/proposicoessp_proposicoes.json

# Importar com configura√ß√µes espec√≠ficas
python importer.py output/proposicoessp_proposicoes.json --max-tokens 4000 --overlap 200
```

### Funcionalidades do Importer

- **Chunking inteligente**: Divide textos longos em chunks baseados em tokens
- **Deduplica√ß√£o**: Evita importar dados duplicados usando UUIDs
- **Progress bar**: Mostra progresso da importa√ß√£o
- **Controle de tokens**: Configura tamanho m√°ximo de chunks para embeddings

## üîß Configura√ß√µes Avan√ßadas

### Modificar Pipelines

Em `settings.py`, voc√™ pode ajustar a ordem e configura√ß√£o dos pipelines:

```python
ITEM_PIPELINES = {
    "assessorai_crawler.pipelines.ValidationPipeline": 100,      # Valida√ß√£o
    "assessorai_crawler.pipelines.JsonWriterSinglePipeline": 300, # Escrita JSON
}
```

### Adicionar Novos Pipelines

Crie novos pipelines em `pipelines.py`:

```python
class CustomPipeline:
    def process_item(self, item, spider):
        # Sua l√≥gica personalizada
        return item
```

### Debug e Logs

Configure logs em `settings.py`:

```python
# N√≠vel de log
LOG_LEVEL = 'INFO'  # DEBUG, INFO, WARNING, ERROR

# Arquivo de log
LOG_FILE = 'scrapy.log'
```

## üß™ Testando Novos Spiders

### 1. Teste B√°sico

```bash
# Teste seco (sem executar)
scrapy check proposicoes[uf]

# Teste com poucos itens
scrapy crawl proposicoes[uf] -s CLOSESPIDER_ITEMCOUNT=10
```

### 2. Valida√ß√£o de Dados

```bash
# Verificar se JSON foi gerado
ls -la output/

# Validar estrutura do JSON
python -m json.tool output/proposicoes[uf]_proposicoes.json
```

### 3. Debug de Items

Adicione logs no seu spider:

```python
def parse(self, response):
    for item in super().parse(response):
        self.logger.info(f"Item processado: {item['title']}")
        yield item
```

## üìã Checklist para Novo Estado

### Fase 1: An√°lise e Planejamento
- [ ] Identificar site oficial da casa legislativa
- [ ] Encontrar se√ß√£o de proposi√ß√µes/projetos de lei
- [ ] Analisar estrutura da p√°gina de listagem
- [ ] Identificar sistema de pagina√ß√£o
- [ ] Verificar se requer JavaScript (SPA)
- [ ] Testar seletores com `scrapy shell`

### Fase 2: Desenvolvimento
- [ ] Criar arquivo `proposicoes[uf].py` no diret√≥rio `spiders/`
- [ ] Definir `name`, `house`, `uf` e configura√ß√µes b√°sicas
- [ ] Implementar `parse()` para listagem
- [ ] Implementar `parse_proposicao()` para p√°ginas individuais
- [ ] Implementar extra√ß√£o de texto completo
- [ ] Implementar convers√£o para markdown

### Fase 3: Testes
- [ ] Testar spider com poucos items (`CLOSESPIDER_ITEMCOUNT=5`)
- [ ] Validar extra√ß√£o de todos os campos obrigat√≥rios
- [ ] Verificar qualidade da convers√£o para markdown
- [ ] Testar pagina√ß√£o completa
- [ ] Verificar tratamento de erros

### Fase 4: Valida√ß√£o
- [ ] Executar coleta completa
- [ ] Validar JSON de sa√≠da
- [ ] Verificar URLs p√∫blicas funcionais
- [ ] Testar importa√ß√£o no Weaviate
- [ ] Documentar peculiaridades do estado

### Fase 5: Documenta√ß√£o
- [ ] Documentar seletores espec√≠ficos usados
- [ ] Documentar estrutura particular do site
- [ ] Documentar problemas encontrados e solu√ß√µes
- [ ] Atualizar README se necess√°rio

## üêõ Problemas Comuns

### Problemas de Seletores CSS/XPath

**Seletores n√£o encontram elementos:**
```python
# ‚ùå Problema: Seletor muito espec√≠fico
response.css('div.container > div.content > table.proposicoes > tr > td > a')

# ‚úÖ Solu√ß√£o: Seletor mais gen√©rico
response.css('a[href*="proposicao"]')
```

**Elementos carregados via JavaScript:**
```python
# ‚ùå Problema: Conte√∫do n√£o existe no HTML inicial
response.css('.proposicao-dinamica')  # Retorna vazio

# ‚úÖ Solu√ß√£o: Usar Selenium
from selenium import webdriver
driver = webdriver.Chrome()
driver.get(response.url)
# Aguardar carregamento e extrair
```

### Problemas de Encoding

**Caracteres especiais quebrados:**
```python
# ‚úÖ Solu√ß√£o: Configurar encoding correto
def parse(self, response):
    response = response.replace(encoding='utf-8')
    # ... resto do c√≥digo
```

### Problemas de Rate Limiting

**Site bloqueia requests r√°pidos:**
```python
# ‚úÖ Configurar delay no settings.py
DOWNLOAD_DELAY = 2  # 2 segundos entre requests
RANDOMIZE_DOWNLOAD_DELAY = 0.5  # Randomizar at√© 50%

# Ou no spider individual
custom_settings = {
    'DOWNLOAD_DELAY': 3,
    'CONCURRENT_REQUESTS': 1
}
```

### Problemas com PDFs

**PDF corrompido ou protegido:**
```python
def extract_pdf_safely(pdf_url):
    try:
        import pdfplumber
        response = requests.get(pdf_url)
        with pdfplumber.open(BytesIO(response.content)) as pdf:
            return "\n".join(page.extract_text() for page in pdf.pages)
    except Exception as e:
        # Fallback para OCR se necess√°rio
        self.logger.warning(f"PDF extraction failed: {e}")
        return self.extract_pdf_with_ocr(pdf_url)
```

### Problemas de Valida√ß√£o

**Campos obrigat√≥rios faltando:**
```python
# Verificar no pipeline se campos essenciais existem
def process_item(self, item, spider):
    required_fields = ['title', 'house', 'url', 'full_text']
    missing = [f for f in required_fields if not item.get(f)]
    
    if missing:
        spider.logger.warning(f"Missing fields: {missing}")
        # Decidir se descartar ou preencher com default
        for field in missing:
            item[field] = 'N/A'  # ou raise DropItem()
    
    return item
```

### Problemas de Mem√≥ria

**Spider consome muita mem√≥ria:**
```python
# ‚úÖ Processar itens em lotes menores
custom_settings = {
    'CONCURRENT_REQUESTS': 1,
    'CLOSESPIDER_ITEMCOUNT': 1000,  # Parar ap√≥s 1000 itens
}

# Ou usar generator para texto muito grande
def extract_large_text(self, response):
    for chunk in self.process_text_in_chunks(response):
        yield chunk
```

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/novo-estado`)
3. Teste thoroughly o novo spider
4. Commit suas mudan√ßas (`git commit -am 'Add spider for XX state'`)
5. Push para a branch (`git push origin feature/novo-estado`)
6. Abra um Pull Request

## üìù Notas Importantes

- **Dados sens√≠veis**: Nunca commite arquivos `.env` ou chaves de API
- **Rate limiting**: Respeite os limites das APIs e sites (use `DOWNLOAD_DELAY`)
- **Robots.txt**: Sempre verifique e respeite o arquivo robots.txt do site
- **User-Agent**: Configure um User-Agent identific√°vel e respeitoso
- **Testes**: Sempre teste com poucos items antes de executar coleta completa
- **URLs p√∫blicas**: Verifique se as URLs extra√≠das s√£o acess√≠veis publicamente  
- **Backup de dados**: Fa√ßa backup dos JSONs gerados antes de reprocessar
- **Monitoramento**: Sites podem mudar estrutura - monitore falhas regularmente
- **Legalidade**: Verifique se o crawling est√° em conformidade com os termos de uso
- **Performance**: Use `CONCURRENT_REQUESTS` e `DOWNLOAD_DELAY` apropriados

### Boas Pr√°ticas de Desenvolvimento

```python
# ‚úÖ Sempre use try/catch para extra√ß√£o
def extract_safely(self, response, selector, default=''):
    try:
        return response.css(selector).get('').strip()
    except Exception as e:
        self.logger.warning(f"Failed to extract {selector}: {e}")
        return default

# ‚úÖ Valide dados antes de salvar
def validate_item(self, item):
    if not item.get('title'):
        return False
    if not item.get('full_text') or len(item['full_text']) < 100:
        return False
    return True

# ‚úÖ Use logs informativos
def parse_proposicao(self, response):
    self.logger.info(f"Processing: {response.url}")
    item = self.extract_item(response)
    
    if self.validate_item(item):
        self.logger.info(f"Extracted: {item['title']}")
        yield item
    else:
        self.logger.warning(f"Invalid item from: {response.url}")
```

## üìö Recursos √öteis

- [Documenta√ß√£o do Scrapy](https://docs.scrapy.org/)
- [Weaviate Documentation](https://weaviate.io/developers/weaviate)
- [OpenAI API Documentation](https://platform.openai.com/docs)
