# AssessorAI Crawler

Sistema de crawler para extraÃ§Ã£o de proposiÃ§Ãµes legislativas de assembleias estaduais brasileiras, desenvolvido com Scrapy e integraÃ§Ã£o com Weaviate.

## ğŸ—ï¸ Arquitetura do Projeto

```
assessorai_crawler/
â”œâ”€â”€ assessorai_crawler/          # CÃ³digo principal do Scrapy
â”‚   â”œâ”€â”€ spiders/                 # Spiders para cada estado
â”‚   â”‚   â”œâ”€â”€ proposicoeslegislapi.py  # Spider base (classe pai)
â”‚   â”‚   â”œâ”€â”€ proposicoessp.py         # SÃ£o Paulo
â”‚   â”‚   â”œâ”€â”€ proposicoesmg.py         # Minas Gerais
â”‚   â”‚   â””â”€â”€ ...                      # Outros estados
â”‚   â”œâ”€â”€ items.py                 # DefiniÃ§Ã£o dos dados estruturados
â”‚   â”œâ”€â”€ pipelines.py             # Processamento e validaÃ§Ã£o dos dados
â”‚   â”œâ”€â”€ settings.py              # ConfiguraÃ§Ãµes do Scrapy
â”‚   â””â”€â”€ utils.py                 # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ output/                      # JSONs gerados pelos crawlers
â”œâ”€â”€ importer.py                  # Script para importar dados no Weaviate
â”œâ”€â”€ requirements.txt             # DependÃªncias Python
â””â”€â”€ .env                        # VariÃ¡veis de ambiente
```

## ğŸš€ ConfiguraÃ§Ã£o do Ambiente

### 1. InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone <repo-url>
cd assessorai_crawler

# Crie e ative um ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows

# Instale as dependÃªncias
pip install -r requirements.txt
```

### 2. ConfiguraÃ§Ã£o das VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# Weaviate Configuration
WEAVIATE_URL="your-weaviate-cluster-url"
WEAVIATE_APIKEY="your-weaviate-api-key"
WEAVIATE_CLASS="Bill"

# OpenAI Configuration (para embeddings)
OPENAI_APIKEY="your-openai-api-key"
```

## ğŸ“Š Como Funciona

### 1. Estrutura de Dados

O projeto usa o item `ProposicaoItem` definido em `items.py`:

```python
ProposicaoItem:
- title: str          # TÃ­tulo da proposiÃ§Ã£o
- house: str          # Casa legislativa
- type: str           # Tipo (PL, PEC, etc.)
- number: int         # NÃºmero da proposiÃ§Ã£o
- year: int           # Ano
- author: list        # Lista de autores
- subject: str        # Ementa/assunto
- full_text: str      # Texto completo
- url: str            # URL pÃºblica
- uuid: str           # Identificador Ãºnico
- scraped_at: str     # Timestamp da coleta
```

### 2. Pipeline de Processamento

1. **ValidationPipeline**: Valida campos obrigatÃ³rios
2. **JsonWriterSinglePipeline**: Salva todos os itens em um Ãºnico JSON

## ğŸ•·ï¸ Como Desenvolver um Novo Crawler Web

### Metodologia: Do Site Ã  Estrutura de Dados

1. **ğŸ” Encontre a pÃ¡gina da casa legislativa**
   - Identifique o site oficial (ex: `www.al[uf].gov.br`)
   - Localize a seÃ§Ã£o de "ProposiÃ§Ãµes", "Projetos de Lei" ou similar

2. **ğŸ“‹ Encontre a pÃ¡gina que lista os projetos**
   - Busque por pÃ¡ginas de listagem (ex: `/proposicoes`, `/projetos`)
   - Analise a paginaÃ§Ã£o e filtros disponÃ­veis

3. **ğŸ”— Itere pela pÃ¡gina, buscando links para projetos individuais**
   - Identifique os seletores CSS/XPath dos links
   - Colete metadados bÃ¡sicos da listagem (tÃ­tulo, autor, data)

4. **ğŸ’¾ Armazene as variÃ¡veis necessÃ¡rias**
   - TÃ­tulo da proposiÃ§Ã£o
   - Tipo e nÃºmero (PL, PEC, etc.)
   - Autor(es)
   - Data de apresentaÃ§Ã£o
   - Ementa/assunto
   - URL pÃºblica

5. **ğŸ“„ FaÃ§a download da Ã­ntegra e converta para markdown**
   - Acesse pÃ¡gina individual do projeto
   - Extraia o texto completo (PDF, HTML, DOC)
   - Converta para markdown limpo

### Passo 1: Estrutura BÃ¡sica do Spider

```python
# assessorai_crawler/spiders/proposicoes[uf].py
import scrapy
import hashlib
from datetime import datetime
from urllib.parse import urljoin
from ..items import ProposicaoItem

class Proposicoes[CASA]Spider(scrapy.Spider):
    name = 'proposicoes[casa]'
    house = 'Nome da Casa Legislativa'
    allowed_domains = ['www.[site da casa].gov.br']
    start_urls = ['https://www.[site da casa].gov.br/proposicoes']
    
    def parse(self, response):
        """Parse da pÃ¡gina de listagem de proposiÃ§Ãµes"""
        # Extrair links para proposiÃ§Ãµes individuais
        proposicao_links = response.css('selector-para-links::attr(href)').getall()
        
        for link in proposicao_links:
            full_url = urljoin(response.url, link)
            yield response.follow(full_url, self.parse_proposicao)
        
        # PaginaÃ§Ã£o
        next_page = response.css('selector-proxima-pagina::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_proposicao(self, response):
        """Parse da pÃ¡gina individual da proposiÃ§Ã£o"""
        item = ProposicaoItem()
        
        # Extrair dados bÃ¡sicos
        item['title'] = response.css('h1.titulo::text').get('').strip()
        item['house'] = self.house
        item['url'] = response.url
        
        # Extrair tipo e nÃºmero do tÃ­tulo
        title_parts = item['title'].split()
        item['type'] = title_parts[0] if title_parts else ''
        
        # Extrair nÃºmero e ano (formato: "123/2024")
        if len(title_parts) > 1:
            try:
                num_year = title_parts[1].split('/')
                item['number'] = int(num_year[0])
                item['year'] = int(num_year[1])
            except (ValueError, IndexError):
                item['number'] = None
                item['year'] = None
        
        # Extrair outros campos
        item['author'] = self.extract_authors(response)
        item['subject'] = response.css('.ementa::text').get('').strip()
        item['presentation_date'] = self.extract_date(response)
        
        # Extrair texto completo
        texto_completo = self.extract_full_text(response)
        item['full_text'] = self.convert_to_markdown(texto_completo)
        item['length'] = len(item['full_text'] or '')
        
        # Metadados
        item['uuid'] = hashlib.md5(item['title'].encode('utf-8')).hexdigest()
        item['scraped_at'] = datetime.now().isoformat()
        
        yield item
    
    def extract_authors(self, response):
        """Extrai lista de autores"""
        authors_text = response.css('.autores::text').get('')
        return [a.strip() for a in authors_text.split(',') if a.strip()]
    
    def extract_date(self, response):
        """Extrai data de apresentaÃ§Ã£o"""
        date_text = response.css('.data-apresentacao::text').get('')
        # Implementar parsing de data especÃ­fico do site
        return date_text.strip()
    
    def extract_full_text(self, response):
        """Extrai texto completo da proposiÃ§Ã£o"""
        # MÃ©todo 1: Texto direto na pÃ¡gina
        full_text = response.css('.texto-completo').get()
        if full_text:
            return full_text
        
        # MÃ©todo 2: Link para PDF/DOC
        pdf_link = response.css('a[href*=".pdf"]::attr(href)').get()
        if pdf_link:
            # Fazer request para PDF e processar (ver seÃ§Ã£o de bibliotecas)
            pass
        
        return ''

```

### Passo 2: Bibliotecas Ãšteis

Adicione ao `requirements.txt`:

```txt
# Parsing e extraÃ§Ã£o
lxml                    # Parser XML/HTML rÃ¡pido

# ConversÃ£o de documentos
markitdown              # https://github.com/microsoft/markitdown

# Processamento de texto
bleach                  # Limpeza de HTML
textract                # ExtraÃ§Ã£o de texto de vÃ¡rios formatos

# Utilidades web
requests-html           # Requests com suporte a JavaScript
selenium                # AutomaÃ§Ã£o de browser (para SPAs)
playwright              # Alternativa moderna ao Selenium
```

## ğŸƒâ€â™‚ï¸ Executando os Crawlers

### Executar um Spider EspecÃ­fico

```bash
# Executar spider de SÃ£o Paulo
scrapy crawl proposicoessp

# Executar spider de Minas Gerais
scrapy crawl proposicoesmg

# Ver lista de todos os spiders
scrapy list
```

### Executar com ConfiguraÃ§Ãµes EspecÃ­ficas

```bash
# Salvar em formato especÃ­fico
scrapy crawl proposicoessp -o output/sp_dados.json

# Limitar nÃºmero de itens (para testes)
scrapy crawl proposicoessp -s CLOSESPIDER_ITEMCOUNT=10

# Configurar delay entre requests
scrapy crawl proposicoessp -s DOWNLOAD_DELAY=2
```

### Exemplos PrÃ¡ticos de Desenvolvimento

```bash
# 1. Criar novo spider interativamente
scrapy genspider proposicoesgo www.assembleia.go.gov.br

# 2. Testar seletores com scrapy shell
scrapy shell "https://www.assembleia.go.gov.br/proposicoes"

# 3. Debug especÃ­fico de uma pÃ¡gina
scrapy shell "https://www.assembleia.go.gov.br/proposicao/12345"

# 4. Executar com configuraÃ§Ãµes de desenvolvimento
scrapy crawl proposicoesgo \
  -s DOWNLOAD_DELAY=1 \
  -s CLOSESPIDER_ITEMCOUNT=5 \
  -L DEBUG
```

### Comandos Ãšteis no Scrapy Shell

```python
# No scrapy shell, use estes comandos para testar:

# Testar seletores CSS
response.css('a.proposicao-link').getall()
response.css('h1.titulo::text').get()

# Testar XPath
response.xpath('//a[contains(@href, "proposicao")]/@href').getall()

# Seguir link e testar
fetch('https://site.gov.br/proposicao/123')
response.css('.texto-completo::text').get()

# Testar regex
import re
title = "PL 123/2024"
match = re.match(r'(\w+)\s+(\d+)/(\d+)', title)
if match:
    print(f"Tipo: {match.group(1)}, NÃºmero: {match.group(2)}, Ano: {match.group(3)}")
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Modificar Pipelines

Em `settings.py`, vocÃª pode ajustar a ordem e configuraÃ§Ã£o dos pipelines:

```python
ITEM_PIPELINES = {
    "assessorai_crawler.pipelines.ValidationPipeline": 100,      # ValidaÃ§Ã£o
    "assessorai_crawler.pipelines.JsonWriterSinglePipeline": 300, # Escrita JSON
}
```

## ğŸ“‹ Checklist para Novo Estado

### Fase 1: AnÃ¡lise e Planejamento
- [ ] Identificar site oficial da casa legislativa
- [ ] Encontrar seÃ§Ã£o de proposiÃ§Ãµes/projetos de lei
- [ ] Analisar estrutura da pÃ¡gina de listagem
- [ ] Identificar sistema de paginaÃ§Ã£o
- [ ] Verificar se requer JavaScript (SPA)
- [ ] Testar seletores com `scrapy shell`

### Fase 2: Desenvolvimento
- [ ] Criar arquivo `proposicoes[casa].py` no diretÃ³rio `spiders/`
- [ ] Definir `name` , `house` e configuraÃ§Ãµes bÃ¡sicas
- [ ] Implementar `parse()` para listagem
- [ ] Implementar `parse_proposicao()` para pÃ¡ginas individuais
- [ ] Implementar extraÃ§Ã£o de texto completo
- [ ] Implementar conversÃ£o para markdown

### Fase 3: Testes
- [ ] Testar spider com poucos items (`CLOSESPIDER_ITEMCOUNT=5`)
- [ ] Validar extraÃ§Ã£o de todos os campos obrigatÃ³rios
- [ ] Verificar qualidade da conversÃ£o para markdown
- [ ] Testar paginaÃ§Ã£o completa
- [ ] Verificar tratamento de erros

### Fase 4: ValidaÃ§Ã£o
- [ ] Executar coleta completa
- [ ] Validar JSON de saÃ­da
- [ ] Verificar URLs pÃºblicas funcionais
- [ ] Testar importaÃ§Ã£o no Weaviate
- [ ] Documentar peculiaridades do estado

## ğŸ› Problemas Comuns

### Problemas de Seletores CSS/XPath

**Seletores nÃ£o encontram elementos:**
```python
# âŒ Problema: Seletor muito especÃ­fico
response.css('div.container > div.content > table.proposicoes > tr > td > a')

# âœ… SoluÃ§Ã£o: Seletor mais genÃ©rico
response.css('a[href*="proposicao"]')
```

**Elementos carregados via JavaScript:**
```python
# âŒ Problema: ConteÃºdo nÃ£o existe no HTML inicial
response.css('.proposicao-dinamica')  # Retorna vazio

# âœ… SoluÃ§Ã£o: Usar Selenium
from selenium import webdriver
driver = webdriver.Chrome()
driver.get(response.url)
# Aguardar carregamento e extrair
```

### Problemas de Encoding

**Caracteres especiais quebrados:**
```python
# âœ… SoluÃ§Ã£o: Configurar encoding correto
def parse(self, response):
    response = response.replace(encoding='utf-8')
    # ... resto do cÃ³digo
```

### Problemas de Rate Limiting

**Site bloqueia requests rÃ¡pidos:**
```python
# âœ… Configurar delay no settings.py
DOWNLOAD_DELAY = 2  # 2 segundos entre requests
RANDOMIZE_DOWNLOAD_DELAY = 0.5  # Randomizar atÃ© 50%

# Ou no spider individual
custom_settings = {
    'DOWNLOAD_DELAY': 3,
    'CONCURRENT_REQUESTS': 1
}
```

## ğŸ“š Recursos Ãšteis

- [DocumentaÃ§Ã£o do Scrapy](https://docs.scrapy.org/)
- [Weaviate Documentation](https://weaviate.io/developers/weaviate)
- [OpenAI API Documentation](https://platform.openai.com/docs)
